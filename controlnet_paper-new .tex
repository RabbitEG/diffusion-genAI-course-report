\documentclass[10pt,twocolumn]{article}

% 基础包
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{times}
\usepackage[margin=0.75in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{cite}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algorithmic}

% 标题信息
\title{\textbf{Training Strategy Analysis for ControlNet: \\Balancing Conditional Controllability and Generalization on Small-Scale Datasets}}

\author{
Hao Zheng$^{1}$, Zhiyi Chen$^{1}$, Rongjie Li$^{1}$ \\
 \\

\date{}

\begin{document}

\maketitle

%==============================================================================
% ABSTRACT
%==============================================================================
\begin{abstract}
ControlNet provides an effective paradigm for incorporating structural conditions into pretrained text-to-image diffusion models. However, the optimal training configuration—particularly regarding which components to freeze and where to inject conditional signals—remains underexplored, especially in small-scale data scenarios. In this work, we systematically investigate the interplay between two key hyperparameters: \texttt{sd\_locked} (whether to freeze the Stable Diffusion backbone) and \texttt{only\_mid\_control} (whether to restrict condition injection to the middle block only). Through ablation studies on the Fill50K dataset, we identify four representative configurations and analyze their convergence behavior, generation quality, and generalization capability. Our experiments reveal that Configuration D (\texttt{sd\_locked=False}, \texttt{only\_mid\_control=True}) achieves the best balance among conditional alignment, prior preservation, and generalization—exhibiting emergent convergence around 2400 steps while avoiding the overfitting observed in other configurations. We provide mechanistic explanations from optimization dynamics and gradient pathway perspectives, offering practical guidelines for training controllable diffusion models under data-limited conditions.
\end{abstract}

\textbf{Keywords:} ControlNet, Diffusion Models, Conditional Image Generation, Training Strategies, Ablation Study

%==============================================================================
% 1. INTRODUCTION
%==============================================================================
\section{Introduction}

Text-to-image diffusion models such as Stable Diffusion \cite{rombach2022high} have demonstrated remarkable capabilities in generating high-quality images from textual descriptions. However, precise spatial control over the generated content—such as specifying exact poses, edges, or depth layouts—remains challenging when relying solely on text prompts. ControlNet \cite{zhang2023adding} addresses this limitation by introducing a trainable copy of the encoder blocks that processes structural conditions (e.g., Canny edges, human poses, depth maps) and modulates the frozen backbone through zero-initialized convolutions.

While the original ControlNet paper establishes the general framework, several practical questions arise when applying this approach to domain-specific applications with limited training data:

\begin{itemize}
    \item Should the Stable Diffusion backbone remain frozen during training, or can selective unfreezing improve adaptation to new condition distributions?
    \item Is it beneficial to inject conditional signals at all decoder levels, or does restricting injection to specific layers yield better generalization?
    \item How do these choices interact with each other and with dataset scale?
\end{itemize}

In this paper, we present a systematic ablation study addressing these questions. Using the Fill50K dataset as a testbed, we evaluate four configurations spanning the combinatorial space of \texttt{sd\_locked} $\in \{\text{True}, \text{False}\}$ and \texttt{only\_mid\_control} $\in \{\text{True}, \text{False}\}$. Our key contributions are:

\begin{enumerate}
    \item \textbf{Empirical characterization} of convergence patterns, generation quality, and overfitting tendencies across configurations.
    \item \textbf{Identification of Configuration D} (\texttt{sd\_locked=False}, \texttt{only\_mid\_control=True}) as optimal for small-data scenarios, achieving high-quality generation at approximately 2400 steps.
    \item \textbf{Mechanistic analysis} explaining the observed behaviors through the lens of effective capacity, gradient pathway constraints, and loss landscape geometry.
    \item \textbf{Practical guidelines} for training ControlNet variants under data-limited conditions.
\end{enumerate}

%==============================================================================
% 2. PROBLEM FORMULATION
%==============================================================================
\section{Problem Formulation}

We formalize the research questions motivating our experimental investigation. Each problem statement corresponds directly to hypotheses tested in our ablation study.

\subsection{Problem 1: Backbone Freezing Trade-off}

The standard ControlNet training protocol freezes all Stable Diffusion parameters (\texttt{sd\_locked=True}), training only the copied encoder blocks and zero-convolution connectors. This preserves the pretrained generative prior but may limit the model's ability to adapt to condition distributions that differ significantly from natural images.

\textbf{Question:} Under what circumstances does unfreezing the backbone (\texttt{sd\_locked=False}) improve conditional alignment without catastrophic forgetting of the generative prior?

\textbf{Addressed in:} Configurations A vs. B (Section~\ref{sec:results}), with mechanistic analysis in Section~\ref{sec:discussion}.

\subsection{Problem 2: Condition Injection Depth}

ControlNet's default architecture injects conditional features at multiple levels of the U-Net decoder through skip connections. An alternative is to restrict injection to the middle block only (\texttt{only\_mid\_control=True}), hypothetically reducing the risk of low-level feature interference.

\textbf{Question:} Does restricting condition injection to the semantic bottleneck layer improve generalization on small datasets by acting as an implicit regularizer?

\textbf{Addressed in:} Configurations A vs. C and B vs. D (Section~\ref{sec:results}), with gradient pathway analysis in Section~\ref{sec:discussion}.

\subsection{Problem 3: Emergent Convergence Dynamics}

Preliminary observations suggest that ControlNet training exhibits non-monotonic behavior: models may maintain near-baseline outputs for many steps before suddenly achieving strong conditional alignment. Understanding this phenomenon is crucial for setting appropriate training schedules.

\textbf{Question:} What causes the ``emergent convergence'' pattern, and how do configuration choices affect its timing and stability?

\textbf{Addressed in:} Step-wise quality analysis (Section~\ref{sec:results}) and optimization dynamics discussion (Section~\ref{sec:discussion}).

\subsection{Problem 4: Optimal Configuration for Small Data}

Combining the above considerations, we seek to identify configurations that achieve:
\begin{itemize}
    \item Strong conditional controllability (alignment with input structure)
    \item Preservation of generative prior (realistic textures, coherent semantics)
    \item Robust generalization (no overfitting to training conditions)
\end{itemize}

\textbf{Question:} Which combination of \texttt{sd\_locked} and \texttt{only\_mid\_control} best balances these objectives on datasets of $\sim$50K samples?

\textbf{Addressed in:} Comparative evaluation and final recommendations (Sections~\ref{sec:results}--\ref{sec:conclusion}).

%==============================================================================
% 3. RELATED WORK
%==============================================================================
\section{Related Work}

\subsection{Diffusion Models for Image Generation}

Diffusion probabilistic models \cite{ho2020denoising,song2020score} have emerged as a dominant paradigm for generative modeling, achieving state-of-the-art results on image synthesis benchmarks. Latent diffusion models \cite{rombach2022high} improve efficiency by operating in a compressed latent space, enabling high-resolution generation with reduced computational cost. Stable Diffusion, built on this framework, serves as the backbone for numerous conditional generation methods.

\subsection{Conditional Control in Diffusion Models}

Several approaches have been proposed to enhance spatial controllability in diffusion models:

\textbf{Classifier Guidance} \cite{dhariwal2021diffusion} steers the sampling process using gradients from an external classifier, but requires training separate classifiers for each condition type.

\textbf{Classifier-Free Guidance} \cite{ho2022classifier} eliminates the need for external classifiers by jointly training conditional and unconditional models, now standard in text-to-image systems.

\textbf{T2I-Adapter} \cite{mou2023t2i} introduces lightweight adapter modules that inject spatial conditions without modifying the base model, enabling efficient multi-condition composition.

\textbf{GLIGEN} \cite{li2023gligen} extends diffusion models with grounding capabilities, allowing bounding box and keypoint-based control through gated self-attention layers.

\textbf{IP-Adapter} \cite{ye2023ip} enables image prompt conditioning through decoupled cross-attention, complementing text-based control with visual references.

\subsection{ControlNet Architecture}

ControlNet \cite{zhang2023adding} introduces a ``trainable copy'' paradigm where the encoder blocks of a pretrained U-Net are duplicated and connected to the frozen decoder through zero-initialized convolutions. This design offers several advantages:

\begin{itemize}
    \item \textbf{Prior preservation:} Zero initialization ensures the model starts from the pretrained behavior.
    \item \textbf{Flexible conditioning:} The same architecture handles diverse condition types (edges, poses, depth, segmentation).
    \item \textbf{Training stability:} Freezing the backbone prevents catastrophic forgetting during fine-tuning.
\end{itemize}

The ControlNet formulation can be expressed as:
\begin{equation}
    y_c = F(x; \Theta) + \mathcal{Z}(F(x + \mathcal{Z}(c; \Theta_{z1}); \Theta_c); \Theta_{z2})
\end{equation}
where $F(\cdot; \Theta)$ denotes the frozen backbone, $\Theta_c$ the trainable copy, and $\mathcal{Z}(\cdot; \Theta_z)$ the zero-initialized convolutions.

\subsection{Parameter-Efficient Fine-Tuning}

Our analysis connects to broader work on efficient adaptation of large models:

\textbf{LoRA} \cite{hu2021lora} constrains weight updates to low-rank subspaces, reducing trainable parameters while maintaining adaptation quality.

\textbf{Adapter Tuning} \cite{houlsby2019parameter} inserts small bottleneck modules between frozen layers, enabling task-specific adaptation with minimal parameter overhead.

The \texttt{only\_mid\_control} configuration can be viewed through this lens: by restricting condition injection to the middle block, we implicitly constrain the effective update subspace, achieving regularization effects similar to architectural bottlenecks.

%==============================================================================
% 4. METHODOLOGY
%==============================================================================
\section{Methods}
\subsection{Merging Conditional Control with the Stable Diffusion Backbone}

ControlNet augments a pretrained text-to-image diffusion model by introducing a secondary, learnable pathway that injects spatial conditioning signals into the denoising network while preserving the capabilities of the original backbone (Stable Diffusion). Let $F(\cdot;\Theta)$ denote a pretrained U-Net block that maps a feature map $x \in \mathbb{R}^{h \times w \times c}$ to an output $y = F(x;\Theta)$. To incorporate structural conditions such as edges, poses, or depth maps, ControlNet constructs a \emph{dual-path} architecture: the original block is kept intact as a frozen backbone, and a trainable copy $F(\cdot;\Theta_c)$ is created to process condition-aware features. These two paths are fused through lightweight $1 \times 1$ convolutions, denoted by $Z(\cdot;\Theta_{z})$, which align feature dimensions and modulate the backbone activations with task-specific information.

Given a conditioning feature map $c$, the ControlNet block computes
\begin{equation}
    y_c = F(x;\Theta) + Z\big(F(x + Z(c;\Theta_{z1}); \Theta_c); \Theta_{z2}\big).
\end{equation}
This formulation allows the trainable copy to interpret the conditioning signal and contribute corrective residuals, while the frozen backbone ensures that the pretrained generative behavior is preserved. The connector convolutions $Z(\cdot;\Theta_{z1})$ and $Z(\cdot;\Theta_{z2})$ serve as projection layers that map the conditioning features into the appropriate channel space and then inject the processed representation back into the main feature stream.

The conditioning feature map $c$ itself is derived from a raw conditioning image $c_i \in \mathbb{R}^{512 \times 512 \times 3}$ through a small encoder $E(\cdot)$. This encoder consists of a few strided convolution layers, where the first convolution primarily aligns the spatial resolution with the latent space of Stable Diffusion (typically $64 \times 64$). The goal of $E(\cdot)$ is not to perform deep semantic extraction but to convert the conditioning image into a feature map compatible with the U-Net resolution at which ControlNet operates.

\subsection{Training Considerations: Frozen Backbone and Zero-Initialized Connectors}

To stabilize learning and prevent catastrophic forgetting, all pretrained parameters $\Theta$ of Stable Diffusion are frozen throughout training. Since conditional datasets are often much smaller than the large-scale corpora used to train the original model, freezing the backbone prevents over-specialization and ensures that the core generative abilities remain intact. Only the parameters of the trainable copy $\Theta_c$, the conditioning encoder $E(\cdot)$, and the connector convolutions $\Theta_{z1}, \Theta_{z2}$ are updated.

A second key design choice is the zero-initialization of the connector convolutions. At initialization, both $Z(c;\Theta_{z1})$ and $Z(\cdot;\Theta_{z2})$ output zero for any input, causing the entire ControlNet block to reduce to
\begin{equation}
    y_c = F(x;\Theta),
\end{equation}
which means the model behaves identically to the pretrained Stable Diffusion at the start of training. This avoids injecting untrained, potentially harmful noise into the backbone and allows the influence of the conditional branch to ``grow in'' smoothly as training progresses. Empirically, this strategy leads to a stable optimization process and a characteristic ``sudden convergence'' effect: after a period in which the model behaves like the vanilla backbone, the connector weights learn meaningful transformations that allow the network to abruptly acquire strong conditioning fidelity.

Together, the dual-path structure, frozen backbone, and zero-initialized connector convolutions form a robust and data-efficient architecture. ControlNet is thus able to learn spatially localized, task-specific controls while preserving the expressive power and visual quality of the underlying diffusion model.


\subsection{Conditioning Generation}

In our experiments, we evaluated two conditioning types: \emph{Human Pose} and \emph{HED Boundary}. The human pose condition is generated using an OpenPose-based keypoint detector, which produces a structured skeletal representation that captures the global configuration and articulation of the subject. The HED boundary condition is derived from the Holistically-Nested Edge Detection (HED) model, which extracts smooth, high-resolution edge maps that provide fine-grained structural cues. Both conditioning types are widely used in controllable generation tasks due to their ability to capture complementary spatial information: pose encodes coarse geometry, while HED preserves local boundaries.

For the condition-type experiments, we adopted publicly available pretrained ControlNet models from HuggingFace. These models were originally trained on large-scale general-domain datasets and have demonstrated strong generalization to diverse image conditions. In our replication, both conditioning types yielded high-quality generations, and the model consistently interpreted the structural signals provided by pose skeletons and boundary maps. The results indicate that the pretrained ControlNet weights possess robust representational capacity and that the conditioning mechanism is sufficiently expressive to guide the generation process in a semantically meaningful way.


\subsection{Training Procedure}

We train our models using the Fill50K dataset, a collection of 50{,}000 image--mask pairs originally developed for image inpainting and completion tasks. The dataset contains diverse structural patterns and object boundaries, providing meaningful supervision for conditioning-guided generation even though it is significantly smaller than the large-scale datasets used for training Stable Diffusion. As our generative backbone, we adopt Stable Diffusion v1.5, a latent diffusion model trained on LAION-5B that produces high-quality natural images through a U-Net denoiser operating in a compressed latent space. Its strong visual prior makes it an ideal foundation for studying conditional finetuning behavior.

To understand the effect of architectural choices during finetuning, we performed an ablation study following the configuration conventions in the ControlNet training documentation.\footnote{\url{https://github.com/lllyasviel/ControlNet/blob/main/docs/train.md}} Specifically, we varied two key flags: \texttt{sd\_locked} and \texttt{only\_mid\_control}. These options determine which portions of the Stable Diffusion U-Net are updated during training.

\paragraph{Definition of \texttt{sd\_locked}.}  
When the configuration \texttt{sd\_locked = True}, all parameters of the original Stable Diffusion model are frozen. Only the ControlNet branch (trainable copy, connector convolutions, and condition encoder) receives gradient updates. This configuration preserves the pretrained generative behavior and prevents the model from drifting away from the domain of natural images.  
When \texttt{sd\_locked = False}, the Stable Diffusion U-Net becomes trainable, greatly increasing the number of parameters updated at each iteration. This allows strong adaptation to the conditioning task, but also increases the risk of overfitting and destabilizing the pretrained backbone.

\paragraph{Definition of \texttt{only\_mid\_control}.}  
When the configuration \texttt{only\_mid\_control = True}, ControlNet attaches trainable modules only to the middle block of the U-Net, rather than to all encoder blocks. This substantially reduces the number of trainable parameters, as the skip-connected encoder pathways remain unchanged.  
When \texttt{only\_mid\_control = False}, ControlNet is attached to every encoder block, maximizing control capacity but also enlarging the effective finetuning footprint.

These configurations produce four experimental variants. To ensure column compatibility, we present the results in a compact table:

\begin{table}[h]
\centering
\scriptsize
\begin{tabular}{c|c|c|c|l}
\hline
ExpID & sd\_locked & mid\_only & Sudden Conv. & Behavior \\
\hline
A & True & False & 2700 & Medium quality \\
B & True & True & N/A & Low quality \\
C & False & False & 1500 & Overfitting, forgetting \\
D & False & True & 2400 & High quality, mild overfit \\
\hline
\end{tabular}
\caption{Ablation results of training configurations.}
\label{tab:ablation}
\end{table}

Across all configurations, we consistently observe the ``sudden convergence'' phenomenon: for several training steps, the model behaves similarly to the pretrained backbone, and then at a specific iteration, it abruptly begins to follow conditioning signals and generate structurally aligned images. However, the step at which this convergence occurs varies significantly between training settings, as shown in Table~\ref{tab:ablation}.

When both the Stable Diffusion model and all ControlNet modules are trainable (\texttt{sd\_locked = False}, \texttt{only\_mid\_control = False}), the network receives gradients across a very large parameter space. This accelerates the sudden convergence (1500 steps) and yields high-quality generations early in training. However, this flexibility also leads to severe overfitting: as training continues, images develop blurry or irregular boundaries, and catastrophic forgetting may occur, with some samples collapsing into disordered patterns.

When the backbone is frozen but all ControlNet blocks are active (\texttt{sd\_locked = True}, \texttt{only\_mid\_control = False}), the training process becomes more stable. This is the recommended configuration in the ControlNet paper. The model maintains reasonable image quality and avoids overfitting even at high training iterations. Notably, the generated images tend to inherit aesthetic properties of Stable Diffusion v1.5 itself—such as characteristic textures and shading—indicating that the frozen backbone strongly shapes the model output.

Restricting ControlNet to the middle block while keeping the backbone frozen (\texttt{sd\_locked = True}, \texttt{only\_mid\_control = True}) significantly limits the effective gradient pathways. As a result, we observe no sudden convergence even after 12k steps, and the model fails to learn meaningful structural patterns from the dataset. The limited backpropagation route through the single mid-block likely reduces optimization efficiency.

Interestingly, when the backbone is unfrozen but ControlNet is attached only to the middle block (\texttt{sd\_locked = False}, \texttt{only\_mid\_control = True}), the model becomes more stable than in the fully trainable case. Overfitting still appears but is substantially mitigated, and image quality improves. We hypothesize two contributing factors: (1) the number of trainable parameters under this configuration becomes comparable to the standard setting (\texttt{sd\_locked = True}, \texttt{only\_mid\_control = False}), whereas the fully trainable configuration involves significantly more parameters; and (2) because only a single decoding pathway is controllable, backpropagation becomes more direct, enabling more effective learning of dataset-specific features without destabilizing the entire U-Net.

Taken together, these results highlight the trade-offs between model stability, learning efficiency, and overfitting. Allowing too many components to update accelerates convergence but risks rapid degradation, whereas restricting updates can improve robustness but slow down or even prevent meaningful learning. Based on our observations, it may be advantageous to unfreeze the backbone while attaching ControlNet only to the middle block (\texttt{sd\_locked = False}, \texttt{only\_mid\_control = True}) when finetuning on a small dataset with a strong and distinctive style, which behaves more like a transfer-learning procedure. In contrast, when the pretrained generative properties of Stable Diffusion v1.5 are crucial—such as maintaining realism or preserving its characteristic aesthetic—and the primary goal of finetuning is to teach the model how to interpret a new conditioning modality rather than to change its visual style, the configuration (\texttt{sd\_locked = True}, \texttt{only\_mid\_control = False}) is preferable.
%==============================================================================
% 5. EXPERIMENTAL RESULTS
%==============================================================================
\section{Experimental Results}
\label{sec:results}

\subsection{Configuration Comparison}

Table~\ref{tab:configs} summarizes the four experimental configurations and their key characteristics.

\begin{table}[h]
\centering
\caption{Ablation configurations and observed outcomes.}
\label{tab:configs}
\begin{tabular}{@{}ccccc@{}}
\toprule
Config & \texttt{sd\_locked} & \texttt{only\_mid} & Steps & Quality \\
\midrule
A & True & True & 2700 & Medium \\
B & False & False & -- & Low (Overfit) \\
C & True & False & 1500 & Overfit \\
D & False & True & 2400 & \textbf{High} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Detailed Analysis by Configuration}

\textbf{Configuration A} (\texttt{sd\_locked=True}, \texttt{only\_mid\_control=True}): This conservative setting achieves medium quality with stable convergence around 2700 steps. The frozen backbone ensures prior preservation, while middle-only injection provides implicit regularization. However, the inability to adapt backbone features limits conditional alignment quality.

\textbf{Configuration B} (\texttt{sd\_locked=False}, \texttt{only\_mid\_control=False}): Full unfreezing with multi-level injection dramatically increases effective capacity. While training loss decreases rapidly, validation metrics degrade, indicating severe overfitting. The model memorizes training condition-output pairs rather than learning generalizable mappings.

\textbf{Configuration C} (\texttt{sd\_locked=True}, \texttt{only\_mid\_control=False}): Despite keeping the backbone frozen, multi-level injection still leads to overfitting, with convergence as early as 1500 steps followed by quality degradation. This suggests that injection depth, not just backbone training, contributes to overfitting risk.

\textbf{Configuration D} (\texttt{sd\_locked=False}, \texttt{only\_mid\_control=True}): This configuration achieves the best results. The unfrozen backbone allows adaptation to the condition distribution, while middle-only injection constrains the effective update subspace. Emergent convergence occurs around 2400 steps, producing high-quality outputs with strong generalization.

\subsection{Emergent Convergence Phenomenon}

A notable observation across configurations is the non-linear convergence behavior. Models maintain outputs close to the unconditional baseline for extended periods before exhibiting sudden alignment with input conditions. This ``emergent convergence'' is most pronounced in Configuration D, where:

\begin{itemize}
    \item Steps 0--1500: Minimal deviation from pretrained behavior
    \item Steps 1500--2200: Gradual increase in condition sensitivity
    \item Steps 2200--2400: Rapid alignment with structural conditions
    \item Steps 2400+: Stable high-quality generation
\end{itemize}

Configuration B also shows rapid change but transitions into overfitting rather than stable alignment.

%==============================================================================
% 6. DISCUSSION
%==============================================================================
\section{Discussion}
\label{sec:discussion}

We provide mechanistic explanations for the observed phenomena from three complementary perspectives: optimization dynamics, gradient pathway constraints, and diffusion sampling trajectory modulation.

\subsection{Optimization Dynamics and Loss Geometry}

The emergent convergence pattern can be understood through the lens of non-convex optimization dynamics.

\subsubsection{Gradient Plateau and Critical Point Transition}

When \texttt{sd\_locked=False}, including backbone parameters significantly expands the optimization landscape, increasing the likelihood of traversing flat regions and saddle point neighborhoods. Combined with zero-initialized convolutions that initially contribute near-zero gradients, early training exhibits a ``plateau phase'' where updates accumulate without visible behavioral change.

As training progresses, coupling between the condition branch and backbone strengthens. When gradient accumulation crosses a critical threshold, the model transitions from ``reusing pretrained prior'' to ``condition-alignment-driven'' mode. This can be formalized as escaping from a flat region along Hessian-dominated directions:
\begin{equation}
    \Delta \theta = -\eta \nabla_\theta \mathcal{L} - \frac{\eta^2}{2} H^{-1} \nabla_\theta \mathcal{L} + O(\eta^3)
\end{equation}
Configuration D's convergence at $\sim$2400 steps suggests it operates near the optimal regime where gradient signal strength balances with stability.

\subsubsection{Effective Capacity and Bias-Variance Trade-off}

Configuration B's rapid training loss reduction coupled with validation degradation exemplifies the high-variance overfitting characteristic of small-data regimes. Full-layer injection plus backbone unfreezing dramatically increases effective capacity, causing the model to deviate from the pretrained manifold and memorize training-specific patterns.

Configuration D's \texttt{only\_mid\_control=True} acts as a structural constraint, implementing implicit regularization that:
\begin{itemize}
    \item Allows backbone adaptation to new condition distributions
    \item Prevents shallow/deep layer perturbations that would corrupt the generative prior
\end{itemize}

This can be augmented with explicit condition consistency regularization:
\begin{equation}
    \mathcal{L}_{\text{total}} = \mathcal{L} + \lambda \mathbb{E}\left[ \| E(g(x)) - c \|_2^2 \right]
\end{equation}
where $E(\cdot)$ is a pretrained condition extractor and $g(x)$ the generated output.

\subsection{Gradient Pathway Analysis}

\subsubsection{Low-Rank Subspace Adaptation Effect}

When \texttt{only\_mid\_control=True}, gradients primarily flow through the middle block parameters $W_{\text{mid}}$:
\begin{equation}
    \frac{\partial \mathcal{L}}{\partial W_{\text{mid}}} = \frac{\partial \mathcal{L}}{\partial y} \cdot \frac{\partial y}{\partial W_{\text{mid}}}
\end{equation}
This effectively restricts updates to a lower-dimensional subspace, analogous to LoRA's low-rank constraint. By limiting which modules receive condition-driven updates, we prevent gradient-induced drift in shallow (detail/edge) and deep (semantic/texture) layers, better preserving the pretrained distribution.

\subsubsection{Structural Bottleneck Effect}

The U-Net middle block encodes high-level abstractions (layout, pose, object shapes), while shallow layers handle local edges/textures and deep layers refine output details. Focusing control injection on the middle layer creates a ``structural bottleneck'':
\begin{itemize}
    \item Structural signals (pose, edges) are more transferable under small data
    \item Texture/local patterns are more easily memorized, leading to overfitting
\end{itemize}
This explains Configuration C's fast convergence but poor generalization: multi-level injection allows texture memorization despite backbone freezing.

\subsubsection{Toward Dynamic Injection}

Fixed injection positions may not be optimal across condition types. A learnable gating mechanism could generalize \texttt{only\_mid\_control}:
\begin{equation}
    y = F(x; \Theta) + \sum_{i=1}^{L} \alpha_i \Delta_i(c, F_i(x))
\end{equation}
where $\alpha_i$ are learned injection weights, enabling condition-adaptive modulation.

\subsection{Sampling Trajectory Modulation}

\subsubsection{Phase-Dependent Condition Influence}

In diffusion sampling, early (high-noise) steps determine global layout while late (low-noise) steps refine textures. Configuration D's architecture aligns with this:
\begin{itemize}
    \item Middle-layer injection primarily influences layout decisions.
    \item Unfrozen backbone retains fine-detail generation capability.
\end{itemize}
This prevents the boundary blurring and unrealistic textures observed in overfit configurations.

\subsubsection{Drift Term Dominance Transition}

Modeling diffusion as a conditioned SDE:
\begin{equation}
    dx = f(x, c) \, dt + g(t) \, dw
\end{equation}
When the condition branch is weak, trajectories follow the pretrained prior. As training strengthens condition coupling past a threshold, the condition-dependent drift term dominates, pulling trajectories toward the condition manifold. Configuration D achieves this transition while the middle-block bottleneck prevents destabilization.

\subsubsection{Memory Effects in Overfitting}

Configuration B's overfitting manifests as ``trajectory memorization'': training condition-output pairs are encoded directly into the denoising path, producing artifacts and blurred boundaries on unseen conditions. This represents contamination of the diffusion prior by training-set statistics.

%==============================================================================
% 7. CONCLUSION
%==============================================================================
\section{Conclusion}
\label{sec:conclusion}

We have presented a systematic investigation of ControlNet training configurations, focusing on the interplay between backbone freezing (\texttt{sd\_locked}) and condition injection depth (\texttt{only\_mid\_control}) under small-scale data conditions.

\subsection{Key Findings}

\begin{enumerate}
    \item \textbf{Configuration D is optimal}: Setting \texttt{sd\_locked=False} and \texttt{only\_mid\_control=True} achieves the best balance of conditional controllability, prior preservation, and generalization on the Fill50K dataset.
    
    \item \textbf{Emergent convergence is configuration-dependent}: The timing and stability of the alignment transition varies significantly, with Configuration D exhibiting stable emergence at $\sim$2400 steps.
    
    \item \textbf{Injection depth is as important as backbone freezing}: Configuration C demonstrates that multi-level injection can cause overfitting even with a frozen backbone.
    
    \item \textbf{Middle-block injection provides implicit regularization}: Restricting conditions to the semantic bottleneck constrains effective capacity while preserving adaptation flexibility.
\end{enumerate}

\subsection{Practical Recommendations}

Based on our analysis, we recommend the following strategies for training ControlNet on limited data:

\begin{enumerate}
    \item \textbf{Progressive unfreezing with smooth scheduling}: Begin with frozen backbone, gradually unfreeze layers, and use cosine annealing to smooth the alignment transition.
    
    \item \textbf{Prioritize structural layer injection}: Default to \texttt{only\_mid\_control=True} or implement hierarchical gating with middle-layer emphasis.
    
    \item \textbf{Add consistency regularization}: Consider cycle consistency or condition-extractable consistency losses:
    \begin{equation}
        \mathcal{L}_{\text{cycle}} = \| \text{Extract}(G(c)) - c \|_2^2
    \end{equation}
    
    \item \textbf{Automated hyperparameter search}: Use Bayesian optimization to explore the configuration space of unfreezing degree, injection levels, and learning rate schedules.
\end{enumerate}

\subsection{Future Directions}

Our findings open several avenues for future work:
\begin{itemize}
    \item Extending the analysis to other condition types (depth, segmentation, normal maps)
    \item Developing learnable injection gating mechanisms
    \item Investigating transfer across datasets with distribution shift
    \item Scaling analysis to larger backbone models (SDXL, SD3)
\end{itemize}

The core insight—that effective capacity and injection topology jointly shape optimization dynamics and generalization—provides a foundation for designing more robust controllable generation systems.

\section{Author responsibilities}
\label{sec:formatting}

 \textbf{Hao Zheng}: In charge of model training and experimental work, setting up test platforms for different tasks (such as action transfer, medical synthesis, etc.), comparing and analyzing the performance differences between Baseline (original ControlNet) and improved methods, ensuring the rigor and reliability of the experiments.
\textbf{Zhiyi Chen}: Responsible for preparing and processing training/ testing data and conducting quantitative and qualitative analysis of experimental results, including metric evaluation, visualization result comparison, identifying the strengths and weaknesses of the methods and suggesting improvement directions.
\textbf{Rongpei Li:} Mainly responsible for proposing and implementing architectural improvement plans, including new ControlNet intermediate layer connection strategies, conditional feature fusion module designs, etc., and collaborating on the formulation of training schemes.
%==============================================================================
% ACKNOWLEDGMENTS
%==============================================================================
\section*{Acknowledgments}

%==============================================================================
% REFERENCES
%==============================================================================
\bibliographystyle{plain}
\begin{thebibliography}{99}

\bibitem{rombach2022high}
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., \& Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In \textit{CVPR}, pp. 10684--10695.

\bibitem{zhang2023adding}
Zhang, L., Rao, A., \& Agrawala, M. (2023). Adding conditional control to text-to-image diffusion models. In \textit{ICCV}, pp. 3836--3847.

\bibitem{ho2020denoising}
Ho, J., Jain, A., \& Abbeel, P. (2020). Denoising diffusion probabilistic models. \textit{NeurIPS}, 33, 6840--6851.

\bibitem{song2020score}
Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., \& Poole, B. (2020). Score-based generative modeling through stochastic differential equations. In \textit{ICLR}.

\bibitem{dhariwal2021diffusion}
Dhariwal, P., \& Nichol, A. (2021). Diffusion models beat GANs on image synthesis. \textit{NeurIPS}, 34, 8780--8794.

\bibitem{ho2022classifier}
Ho, J., \& Salimans, T. (2022). Classifier-free diffusion guidance. \textit{arXiv preprint arXiv:2207.12598}.

\bibitem{mou2023t2i}
Mou, C., Wang, X., Xie, L., Zhang, J., Qi, Z., Shan, Y., \& Qie, X. (2023). T2I-Adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. \textit{arXiv preprint arXiv:2302.08453}.

\bibitem{li2023gligen}
Li, Y., Liu, H., Wu, Q., Mu, F., Yang, J., Gao, J., Li, C., \& Lee, Y. J. (2023). GLIGEN: Open-set grounded text-to-image generation. In \textit{CVPR}, pp. 22511--22521.

\bibitem{ye2023ip}
Ye, H., Zhang, J., Liu, S., Han, X., \& Yang, W. (2023). IP-Adapter: Text compatible image prompt adapter for text-to-image diffusion models. \textit{arXiv preprint arXiv:2308.06721}.

\bibitem{hu2021lora}
Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., \& Chen, W. (2021). LoRA: Low-rank adaptation of large language models. In \textit{ICLR}.

\bibitem{houlsby2019parameter}
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., \& Gelly, S. (2019). Parameter-efficient transfer learning for NLP. In \textit{ICML}, pp. 2790--2799.

\end{thebibliography}

\end{document}
