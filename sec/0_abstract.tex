\begin{abstract}
ControlNet provides an effective paradigm for incorporating structural conditions into pretrained text-to-image diffusion models. However, the optimal training configuration—particularly regarding which components to freeze and where to inject conditional signals—remains underexplored, especially in small-scale data scenarios. In this work, we systematically investigate the interplay between two key hyperparameters: \texttt{sd\_locked} (whether to freeze the Stable Diffusion backbone) and \texttt{only\_mid\_control} (whether to restrict condition injection to the middle block only). Through ablation studies on the Fill50K dataset, we identify four representative configurations and analyze their convergence behavior, generation quality, and generalization capability. Our experiments reveal that Configuration D (\texttt{sd\_locked=False}, \texttt{only\_mid\_control=True}) achieves the best balance among conditional alignment, prior preservation, and generalization—exhibiting emergent convergence around 2400 steps while avoiding the overfitting observed in other configurations. We provide mechanistic explanations from optimization dynamics and gradient pathway perspectives, offering practical guidelines for training controllable diffusion models under data-limited conditions.
\end{abstract}

\textbf{Keywords:} ControlNet, Diffusion Models, Conditional Image Generation, Training Strategies
