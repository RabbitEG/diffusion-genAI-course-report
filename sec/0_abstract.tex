\begin{abstract}
In recent years, diffusion models represented by Stable Diffusion~\cite{Rombach2022LDM} have achieved great success in the field of text-to-image generation. The emergence of ControlNet~\cite{Zhang2023ControlNet} has greatly enhanced the controllability of the generation process by introducing external structural conditions such as attitude maps and edge maps. However, most of the existing studies follow the standard paradigm of freezing the backbone of pre-trained models, and there is insufficient exploration of the optimal training strategies in different fine-tuning scenarios. This paper aims to systematically study the training strategies of ControlNet, especially the impact of the parameter update (sdlocked) of the pre-trained U-Net backbone and the injection range of the control module (onlymidcontrol) on the model performance. We conducted a series of ablation experiments on the Fill50K dataset under the conditions of pose and edge maps. The experimental results reveal a key finding: Compared with the standard practice of completely freezing the backbone, adopting a strategy of "unlocking the backbone and simplifying the control" (that is, unfreezing the Stable Diffusion backbone while only applying the ControlNet module to the middle layer of U-Net) can achieve a better balance on small datasets. This strategy not only accelerates the "emergent convergence" of the model, improves the quality of the generated images and the accuracy of conditional alignment, but also effectively alleviates the severe overfitting and catastrophic forgetting problems that occur when all parameters are completely thawed. Our research indicates that this fine-tuning method similar to transfer learning offers a new and efficient paradigm for training ControlNet on specific styles or small-scale datasets, providing valuable practical guidance for the design and application in the field of controllable image generation.
\end{abstract}
