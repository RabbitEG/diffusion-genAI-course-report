\section{Objectives}
\label{sec:objectives}
This study aims to systematically enhance the image generation effects based on pose maps and edge maps under conditional control using ControlNet. The specific objectives include:

Expanding Application Scenarios: To explore the applicability of ControlNet in a wider range of tasks, such as character action transfer, medical image synthesis, complex scene layout generation, and video keyframe guidance, thereby validating the value of structured conditions across different fields.

Architectural Innovation and Optimization: To investigate improvements in the architectural design of the ControlNet model, including strategies for inserting conditions at intermediate layers of different diffusion models, optimizing the connection methods between the control branch and the backbone network, and developing efficient mechanisms for fusing conditional features with generated features.

Improving Training Strategies: To develop effective training schemes that enhance the model's control capabilities and stability, such as reasonable freezing/thawing strategies for the main model, methods to improve the expression and alignment of condition signals during training, and regularization techniques to boost training stability.

We aim to significantly improve the quality of images generated under target pose or edge conditions, ensuring that the output results more accurately match the given conditions in terms of details and structure, while maintaining the diversity and visual realism of the images.
\subsection{Research Questions}
\label{sec:research_questions}
To achieve the research objectives, this paper will focus on exploring the following key issues:

\begin{enumerate}
    \item Expansion of ControlNet Applications: Besides generating static character images, under the condition of using pose keypoint maps and edge detection maps, in what other generation tasks can ControlNet be expanded? How does it play a role in motion transfer in dynamic videos, synthesis of medical images, generation from scene layout to images, and continuous guidance for video keyframes, and what challenges does it face?
    \item Optimization of Architecture Design: Currently, ControlNet introduces control signals by adding a zero-convolution conditional branch to a pre-trained diffusion model. Is there a more optimal network architecture or fusion mechanism that could improve the efficiency of condition utilization? For example, at which levels (shallow vs. deep, encoder vs. decoder) of the diffusion UNet should the conditional features be injected, and what kind of connection and fusion methods (addition, concatenation, or attention mechanisms) would yield the best generation results and precise condition alignment?
    \item Improvement of Training Strategies: Without compromising the original generative capabilities, how can effective training strategies be designed to enhance ControlNet's sensitivity and control accuracy towards conditions? For instance, in what situations should the weights of the pre-trained diffusion model be frozen, or gradually unfrozen to adapt to new domains? Can the introduction of additional loss functions (such as cycle consistency loss or intermediate feature alignment) improve the match between conditions and outputs, thereby enhancing training stability and generation quality?
\end{enumerate}

\subsection{Related Work}
\label{sec:related_work}
\textbf{T2I-Adapter: A Lightweight Control Module.}
Following ControlNet~\cite{Zhang2023ControlNet}, researchers have explored alternative efficient methods for incorporating structural conditions. T2I-Adapter~\cite{Mou2023T2IAdapter} introduces a lightweight, plug-and-play adapter module that injects external control signals without modifying the original Stable Diffusion architecture. This approach trains small convolutional networks for each condition type (e.g., sketch, edge, depth, keypoints) to encode multi-scale features, which are then added to the feature maps at different layers of Stable Diffusion's U-Net encoder. While the base model remains frozen, only the adapter parameters are updated. With substantially fewer parameters (e.g., 77M compared to ControlNet's 567M), T2I-Adapter enables faster inference while achieving comparable or superior structural and textual alignment on datasets like COCO~\cite{Lin2014COCO}. Additionally, it supports flexible module composition: adapters for different conditions can be weighted and combined during inference to enable multi-condition control without retraining. For instance, merging outputs from sketch and color palette adapters allows simultaneous control over shape and color. The method also employs non-uniform timestep sampling during training to enhance guidance for low-level visual features (e.g., edges, colors). Overall, T2I-Adapter demonstrates an effective pathway for leveraging large models' implicit capabilities through compact modules, offering a valuable complement to the ControlNet framework.

\textbf{Other Structured Conditional Generation Methods.}
Beyond the above, several recent architectures have been proposed to enhance controllability by integrating structural information into diffusion models. For example, GLIGEN~\cite{Li2023GLIGEN} focuses on controlling scene layout and object placement. By incorporating learnable gated units into Stable Diffusion's cross-attention layers, GLIGEN accepts bounding box coordinates and corresponding object labels as additional inputs, enabling precise object positioning. Similar to other methods, it freezes most of the pre-trained weights and trains only a small set of gating and embedding parameters, thereby endowing the model with grounding capabilities. Experiments show that GLIGEN, built upon pre-trained diffusion models, achieves higher image quality and layout accuracy in layout-to-image tasks compared to models trained from scratch. Another related direction is multi-conditional diffusion, where models are trained to accept composite controls (e.g., text, segmentation, edges) in a single forward pass. However, such approaches typically require full retraining or large-scale fine-tuning, incurring high computational costs and potential limitations in generalizing to unseen condition combinations.

\textbf{Training Strategies and Enhanced Control Performance.}
As structural conditional generation methods evolve, improving condition adherence and output quality has become a key research focus. Some studies introduce additional constraint losses during training to strengthen condition-image consistency. For example, a pixel-level cycle consistency loss can be used: pre-trained discriminative models (e.g., edge detectors, segmentation networks) are applied to generated images to extract condition signals, which are then compared with the original input conditions to directly optimize alignment. To mitigate the high computational cost of full diffusion sampling during loss calculation, an efficient single-step perturbation strategy can approximate the generated output for consistency evaluation.

Another line of work explores intermediate feature alignment training. At each denoising step, lightweight convolutional probes are trained to reconstruct input condition maps (e.g., edges or depth) from the U-Net's intermediate features. During training, a consistency loss is computed between the predicted ``pseudo-condition'' from noisy latents and the ground-truth condition, encouraging the model to maintain condition awareness throughout the diffusion process. This strategy embeds control signals more deeply into generation, enhancing structural fidelity and control precision.

In summary, recent advances—spanning model architectures (e.g., ControlNet, T2I-Adapter, GLIGEN) and training strategies (e.g., frozen fine-tuning, adapter composition, consistency constraints)—have continuously advanced the field of structurally conditioned controllable generation. Building upon these works, this study further extends applicable scenarios and proposes improvements for generation tasks under pose and edge map conditions, contributing to enhanced quality and precision in controllable image synthesis.
