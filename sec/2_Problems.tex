\section{Problem Formulation}

We formalize the research questions motivating our experimental investigation. Each problem statement corresponds directly to hypotheses tested in our ablation study.

\paragraph{Problem 1: Backbone Freezing Trade-off.}

The standard ControlNet training protocol freezes all Stable Diffusion parameters (\texttt{sd\_locked=True}), training only the copied encoder blocks and zero-convolution connectors. This preserves the pretrained generative prior but may limit the model's ability to adapt to condition distributions that differ significantly from natural images.

\textbf{Question:} Under what circumstances does unfreezing the backbone (\texttt{sd\_locked=False}) improve conditional alignment without catastrophic forgetting of the generative prior in practice?

\textbf{Addressed in:} Configurations A vs. B (Section~\ref{sec:results}), with mechanistic analysis in Section~\ref{sec:discussion}.

\paragraph{Problem 2: Condition Injection Depth.}

ControlNet's default architecture injects conditional features at multiple levels of the U-Net decoder through skip connections. An alternative is to restrict injection to the middle block only (\texttt{only\_mid\_control=True}), hypothetically reducing the risk of low-level feature interference.

\textbf{Question:} Does restricting condition injection to the semantic bottleneck layer improve generalization on small datasets by acting as an implicit regularizer?

\textbf{Addressed in:} Configurations A vs. C and B vs. D (Section~\ref{sec:results}), with gradient pathway analysis in Section~\ref{sec:discussion}.

\paragraph{Problem 3: Emergent Convergence Dynamics.}

Preliminary observations suggest that ControlNet training exhibits non-monotonic behavior: models may maintain near-baseline outputs for many steps before suddenly achieving strong conditional alignment. Understanding this phenomenon is crucial for setting training schedules.

\textbf{Question:} What causes the ``emergent convergence'' pattern, and how do configuration choices affect its timing and stability?

\textbf{Addressed in:} Step-wise quality analysis (Section~\ref{sec:results}) and optimization dynamics discussion (Section~\ref{sec:discussion}).

\paragraph{Problem 4: Optimal Configuration for Small Data.}

Combining the above considerations, we seek to identify configurations that achieve strong conditional controllability (alignment with input structure), preservation of the generative prior (realistic textures, coherent semantics), and robust generalization (no overfitting to training conditions).

\textbf{Question:} Which combination of \texttt{sd\_locked} and \texttt{only\_mid\_control} best balances these objectives on datasets of $\sim$50K samples?

\textbf{Addressed in:} Comparative evaluation and final recommendations (Sections~\ref{sec:results}--\ref{sec:conclusion}).

%==============================================================================
% 3. RELATED WORK
%==============================================================================
\section{Related Work}

\subsection{Diffusion Models for Image Generation}

Diffusion probabilistic models \cite{ho2020denoising,song2020score} have emerged as a dominant paradigm for generative modeling, achieving state-of-the-art results on image synthesis benchmarks. Latent diffusion models \cite{rombach2022high} improve efficiency by operating in a compressed latent space, enabling high-resolution generation with reduced computational cost. Stable Diffusion, built on this framework, serves as the backbone for numerous conditional generation methods.

\subsection{Conditional Control in Diffusion Models}

Several approaches have been proposed to enhance spatial controllability in diffusion models:

\textbf{Classifier Guidance} \cite{dhariwal2021diffusion} steers the sampling process using gradients from an external classifier, but requires training separate classifiers for each condition type.

\textbf{Classifier-Free Guidance} \cite{ho2022classifier} eliminates the need for external classifiers by jointly training conditional and unconditional models, now standard in text-to-image systems.

\textbf{T2I-Adapter} \cite{mou2023t2i} introduces lightweight adapter modules that inject spatial conditions without modifying the base model, enabling efficient multi-condition composition.

\textbf{GLIGEN} \cite{li2023gligen} extends diffusion models with grounding capabilities, allowing bounding box and keypoint-based control through gated self-attention layers.

\textbf{IP-Adapter} \cite{ye2023ip} enables image prompt conditioning through decoupled cross-attention, complementing text-based control with visual references.

\subsection{ControlNet Architecture}

ControlNet \cite{zhang2023adding} introduces a ``trainable copy'' paradigm where the encoder blocks of a pretrained U-Net are duplicated and connected to the frozen decoder through zero-initialized convolutions. This design offers several advantages: \textbf{prior preservation}, because zero initialization ensures the model starts from the pretrained behavior; \textbf{flexible conditioning}, because the same architecture handles diverse condition types (edges, poses, depth, segmentation); and \textbf{training stability}, because freezing the backbone prevents catastrophic forgetting during fine-tuning.

The ControlNet formulation can be expressed as:
\begin{equation}
    y_c = F(x; \Theta) + \mathcal{Z}(F(x + \mathcal{Z}(c; \Theta_{z1}); \Theta_c); \Theta_{z2})
\end{equation}
where $F(\cdot; \Theta)$ denotes the frozen backbone, $\Theta_c$ the trainable copy, and $\mathcal{Z}(\cdot; \Theta_z)$ the zero-initialized convolutions.

\subsection{Parameter-Efficient Fine-Tuning}

Our analysis connects to broader work on efficient adaptation of large models:

\textbf{LoRA} \cite{hu2021lora} constrains weight updates to low-rank subspaces, reducing trainable parameters while maintaining adaptation quality.

\textbf{Adapter Tuning} \cite{houlsby2019parameter} inserts small bottleneck modules between frozen layers, enabling task-specific adaptation with minimal parameter overhead.

The \texttt{only\_mid\_control} configuration can be viewed through this lens: by restricting condition injection to the middle block, we implicitly constrain the effective update subspace, achieving regularization effects similar to architectural bottlenecks.

%==============================================================================
% 4. METHODOLOGY
%==============================================================================
