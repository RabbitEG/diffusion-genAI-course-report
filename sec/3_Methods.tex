\section{Methods}
\subsection{Merging Conditional Control with the Stable Diffusion Backbone}

ControlNet augments a pretrained text-to-image diffusion model by introducing a secondary, learnable pathway that injects spatial conditioning signals into the denoising network while preserving the capabilities of the original backbone (Stable Diffusion). Let $F(\cdot;\Theta)$ denote a pretrained U-Net block that maps a feature map $x \in \mathbb{R}^{h \times w \times c}$ to an output $y = F(x;\Theta)$. To incorporate structural conditions such as edges, poses, or depth maps, ControlNet constructs a \emph{dual-path} architecture: the original block is kept intact as a frozen backbone, and a trainable copy $F(\cdot;\Theta_c)$ is created to process condition-aware features. These two paths are fused through lightweight $1 \times 1$ convolutions, denoted by $Z(\cdot;\Theta_{z})$, which align feature dimensions and modulate the backbone activations with task-specific information.

Given a conditioning feature map $c$, the ControlNet block computes
\begin{equation}
    y_c = F(x;\Theta) + Z\big(F(x + Z(c;\Theta_{z1}); \Theta_c); \Theta_{z2}\big).
\end{equation}
This formulation allows the trainable copy to interpret the conditioning signal and contribute corrective residuals, while the frozen backbone ensures that the pretrained generative behavior is preserved. The connector convolutions $Z(\cdot;\Theta_{z1})$ and $Z(\cdot;\Theta_{z2})$ serve as projection layers that map the conditioning features into the appropriate channel space and then inject the processed representation back into the main feature stream.

The conditioning feature map $c$ itself is derived from a raw conditioning image $c_i \in \mathbb{R}^{512 \times 512 \times 3}$ through a small encoder $E(\cdot)$. This encoder consists of a few strided convolution layers, where the first convolution primarily aligns the spatial resolution with the latent space of Stable Diffusion (typically $64 \times 64$). The goal of $E(\cdot)$ is not to perform deep semantic extraction but to convert the conditioning image into a feature map compatible with the U-Net resolution at which ControlNet operates.

\subsection{Training Considerations: Frozen Backbone and Zero-Initialized Connectors}

To stabilize learning and prevent catastrophic forgetting, all pretrained parameters $\Theta$ of Stable Diffusion are frozen throughout training. Since conditional datasets are often much smaller than the large-scale corpora used to train the original model, freezing the backbone prevents over-specialization and ensures that the core generative abilities remain intact. Only the parameters of the trainable copy $\Theta_c$, the conditioning encoder $E(\cdot)$, and the connector convolutions $\Theta_{z1}, \Theta_{z2}$ are updated.

A second key design choice is the zero-initialization of the connector convolutions. At initialization, both $Z(c;\Theta_{z1})$ and $Z(\cdot;\Theta_{z2})$ output zero for any input, causing the entire ControlNet block to reduce to
\begin{equation}
    y_c = F(x;\Theta),
\end{equation}
which means the model behaves identically to the pretrained Stable Diffusion at the start of training. This avoids injecting untrained, potentially harmful noise into the backbone and allows the influence of the conditional branch to ``grow in'' smoothly as training progresses. Empirically, this strategy leads to a stable optimization process and a characteristic ``sudden convergence'' effect: after a period in which the model behaves like the vanilla backbone, the connector weights learn meaningful transformations that allow the network to abruptly acquire strong conditioning fidelity.

Together, the dual-path structure, frozen backbone, and zero-initialized connector convolutions form a robust and data-efficient architecture. ControlNet is thus able to learn spatially localized, task-specific controls while preserving the expressive power and visual quality of the underlying diffusion model.


\subsection{Conditioning Generation}

In our experiments, we evaluated two conditioning types: \emph{Human Pose} and \emph{HED Boundary}. The human pose condition is generated using an OpenPose-based keypoint detector, which produces a structured skeletal representation that captures the global configuration and articulation of the subject. The HED boundary condition is derived from the Holistically-Nested Edge Detection (HED) model, which extracts smooth, high-resolution edge maps that provide fine-grained structural cues. Both conditioning types are widely used in controllable generation tasks due to their ability to capture complementary spatial information: pose encodes coarse geometry, while HED preserves local boundaries at fine detail.

For the condition-type experiments, we adopted publicly available pretrained ControlNet models from HuggingFace. These models were originally trained on large-scale general-domain datasets and have demonstrated strong generalization to diverse image conditions. In our replication, both conditioning types yielded high-quality generations, and the model consistently interpreted the structural signals provided by pose skeletons and boundary maps. The results indicate that the pretrained ControlNet weights possess robust representational capacity and that the conditioning mechanism is sufficiently expressive to guide the generation process in a semantically meaningful way.


\subsection{Training Procedure}

We train our models using the Fill50K dataset, a collection of 50{,}000 image--mask pairs developed for image inpainting and completion tasks. The dataset contains diverse structural patterns and object boundaries, providing meaningful supervision for conditioning-guided generation even though it is much smaller than the large-scale datasets used for training Stable Diffusion. As our generative backbone, we adopt Stable Diffusion v1.5, a latent diffusion model trained on LAION-5B that produces high-quality natural images through a U-Net denoiser operating in a compressed latent space. Its strong visual prior makes it an ideal foundation for studying conditional fine-tuning behavior.

To understand the effect of architectural choices during fine-tuning, we performed an ablation study following the configuration conventions in the ControlNet training documentation.\footnote{\url{https://github.com/lllyasviel/ControlNet/blob/main/docs/train.md}} Specifically, we varied two key flags: \texttt{sd\_locked} and \texttt{only\_mid\_control}. These options determine which portions of the Stable Diffusion U-Net are updated during training.

\paragraph{Definition of \texttt{sd\_locked}.}  
When the configuration \texttt{sd\_locked = True}, all parameters of the original Stable Diffusion model are frozen. Only the ControlNet branch (trainable copy, connector convolutions, and condition encoder) receives gradient updates. This configuration preserves the pretrained generative behavior and prevents the model from drifting away from the domain of natural images.  
When \texttt{sd\_locked = False}, the Stable Diffusion U-Net becomes trainable, greatly increasing the number of parameters updated at each iteration. This allows strong adaptation to the conditioning task, but also increases the risk of overfitting and destabilizing the pretrained backbone.

\paragraph{Definition of \texttt{only\_mid\_control}.}  
When the configuration \texttt{only\_mid\_control = True}, ControlNet attaches trainable modules only to the middle block of the U-Net, rather than to all encoder blocks. This substantially reduces the number of trainable parameters, as the skip-connected encoder pathways remain unchanged.  
When \texttt{only\_mid\_control = False}, ControlNet is attached to every encoder block, maximizing control capacity but also enlarging the effective fine-tuning footprint.

These configurations produce four experimental variants. To ensure column compatibility, we present the results in a compact table:

\begin{table}[h]
\caption{Ablation results of training configurations.}
\centering
\scriptsize
\begin{tabular}{c|c|c|c|l}
\hline
ExpID & sd\_locked & mid\_only & Sudden Conv. & Behavior \\
\hline
A & True & False & 2700 & Medium quality \\
B & True & True & N/A & Low quality \\
C & False & False & 1500 & Overfitting, forgetting \\
D & False & True & 2400 & High quality, mild overfit \\
\hline
\end{tabular}
\label{tab:ablation}
\end{table}

Across all configurations, we consistently observe the ``sudden convergence'' phenomenon: for several training steps, the model behaves similarly to the pretrained backbone, and then at a specific iteration, it abruptly begins to follow conditioning signals and generate structurally aligned images. However, the step at which this convergence occurs varies significantly between training settings, as shown in Table~\ref{tab:ablation}.

When both the Stable Diffusion model and all ControlNet modules are trainable (\texttt{sd\_locked = False}, \texttt{only\_mid\_control = False}), the network receives gradients across a very large parameter space. This accelerates the sudden convergence (1500 steps) and yields high-quality generations early in training. However, this flexibility also leads to severe overfitting: as training continues, images develop blurry or irregular boundaries, and catastrophic forgetting may occur, with some samples collapsing into disordered patterns.

When the backbone is frozen but all ControlNet blocks are active (\texttt{sd\_locked = True}, \texttt{only\_mid\_control = False}), the training process becomes more stable. This is the recommended configuration in the ControlNet paper. The model maintains reasonable image quality and avoids overfitting even at high training iterations. Notably, the generated images tend to inherit aesthetic properties of Stable Diffusion v1.5 itself—such as characteristic textures and shading—indicating that the frozen backbone strongly shapes the model output.

Restricting ControlNet to the middle block while keeping the backbone frozen (\texttt{sd\_locked = True}, \texttt{only\_mid\_control = True}) significantly limits the effective gradient pathways. As a result, we observe no sudden convergence even after 12k steps, and the model fails to learn meaningful structural patterns from the dataset. The limited backpropagation route through the single mid-block likely reduces optimization efficiency.

Interestingly, when the backbone is unfrozen but ControlNet is attached only to the middle block (\texttt{sd\_locked = False}, \texttt{only\_mid\_control = True}), the model becomes more stable than in the fully trainable case. Overfitting still appears but is mitigated, and image quality improves. We hypothesize two contributing factors: (1) the number of trainable parameters under this configuration becomes comparable to the standard setting (\texttt{sd\_locked = True}, \texttt{only\_mid\_control = False}), whereas the fully trainable configuration involves significantly more parameters; and (2) because only a single decoding pathway is controllable, backpropagation becomes more direct, enabling more effective learning of dataset-specific features without destabilizing the entire U-Net.

Taken together, these results highlight the trade-offs between model stability, learning efficiency, and overfitting. Allowing too many components to update accelerates convergence but risks rapid degradation, whereas restricting updates can improve robustness but slow down or even prevent meaningful learning. Based on our observations, it may be advantageous to unfreeze the backbone while attaching ControlNet only to the middle block (\texttt{sd\_locked = False}, \texttt{only\_mid\_control = True}) when fine-tuning on a small dataset with a strong and distinctive style, which behaves more like a transfer-learning procedure. In contrast, when the pretrained generative properties of Stable Diffusion v1.5 are crucial—such as maintaining realism or preserving its characteristic aesthetic—and the primary goal of fine-tuning is to teach the model how to interpret a new conditioning modality rather than to change its visual style, the configuration (\texttt{sd\_locked = True}, \texttt{only\_mid\_control = False}) is preferable.
%==============================================================================
% 5. EXPERIMENTAL RESULTS
%==============================================================================
