\bibliographystyle{plain}
\begin{thebibliography}{99}

\bibitem{rombach2022high}
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., \& Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In \textit{CVPR}, pp. 10684--10695.

\bibitem{zhang2023adding}
Zhang, L., Rao, A., \& Agrawala, M. (2023). Adding conditional control to text-to-image diffusion models. In \textit{ICCV}, pp. 3836--3847.

\bibitem{ho2020denoising}
Ho, J., Jain, A., \& Abbeel, P. (2020). Denoising diffusion probabilistic models. \textit{NeurIPS}, 33, 6840--6851.

\bibitem{song2020score}
Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., \& Poole, B. (2020). Score-based generative modeling through stochastic differential equations. In \textit{ICLR}.

\bibitem{dhariwal2021diffusion}
Dhariwal, P., \& Nichol, A. (2021). Diffusion models beat GANs on image synthesis. \textit{NeurIPS}, 34, 8780--8794.

\bibitem{ho2022classifier}
Ho, J., \& Salimans, T. (2022). Classifier-free diffusion guidance. \textit{arXiv preprint arXiv:2207.12598}.

\bibitem{mou2023t2i}
Mou, C., Wang, X., Xie, L., Zhang, J., Qi, Z., Shan, Y., \& Qie, X. (2023). T2I-Adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. \textit{arXiv preprint arXiv:2302.08453}.

\bibitem{li2023gligen}
Li, Y., Liu, H., Wu, Q., Mu, F., Yang, J., Gao, J., Li, C., \& Lee, Y. J. (2023). GLIGEN: Open-set grounded text-to-image generation. In \textit{CVPR}, pp. 22511--22521.

\bibitem{ye2023ip}
Ye, H., Zhang, J., Liu, S., Han, X., \& Yang, W. (2023). IP-Adapter: Text compatible image prompt adapter for text-to-image diffusion models. \textit{arXiv preprint arXiv:2308.06721}.

\bibitem{hu2021lora}
Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., \& Chen, W. (2021). LoRA: Low-rank adaptation of large language models. In \textit{ICLR}.

\bibitem{houlsby2019parameter}
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Gesmundo, A., Attariyan, M., \& Gelly, S. (2019). Parameter-efficient transfer learning for NLP. In \textit{ICML}, pp. 2790--2799.

\end{thebibliography}